Here is the full text of the second PDF (5-2.pdf), transcribed via OCR:

---

**5.2 PARSING AND AMBIGUITY**

We have so far concentrated on the generative aspects of grammars. Given a grammar \( G \), we studied the set of strings that can be derived using \( G \). In cases of practical applications, we are also concerned with the analytical side of the grammar: Given a string \( w \) of terminals, we want to know whether or not \( w \) is in \( L(G) \). If so, we may want to find a derivation of \( w \). An algorithm that can tell us whether \( w \) is in \( L(G) \) is a membership algorithm. The term **parsing** describes finding a sequence of productions by which a \( w \in L(G) \) is derived.

---

**Parsing and Membership**

Given a string \( w \) in \( L(G) \), we can parse it in a rather obvious fashion: We systematically construct all possible (say, leftmost) derivations and see whether any of them match \( w \). Specifically, we start at round one by looking at all productions of the form  

\[S \to x,\]

finding all \( x \) that can be derived from \( S \) in one step. If none of these results in a match with \( w \), we go to the next round, in which we apply all applicable productions to the leftmost variable of every \( x \). This gives us a set of sentential forms, some of them possibly leading to \( w \). On each subsequent round, we again take all leftmost variables and apply all possible productions. It may be that some of these sentential forms can be rejected on the grounds that \( w \) can never be derived from them, but in general, we will have on each round a set of possible sentential forms. After the first round, we have sentential forms that can be derived by applying a single production, after the second round we have the sentential forms that can be derived in two steps, and so on. If \( w \in L(G) \), then it must have a leftmost derivation of finite length. Thus, the method will eventually give a leftmost derivation of \( w \).

For reference below, we will call this **exhaustive search parsing** or **brute force parsing**. It is a form of **top-down parsing**, which we can view as the construction of a derivation tree from the root down.

---

**EXAMPLE 5.7**  
Consider the grammar  
\[ S \rightarrow SS \mid aSb \mid bSa \mid \lambda \]  
and the string \( w = aabb \). Round one gives us  
1. \( S \Rightarrow SS \),  
2. \( S \Rightarrow aSb \),  
3. \( S \Rightarrow bSa \),  
4. \( S \Rightarrow \lambda \).  

The last two of these can be removed from further consideration for obvious reasons. Round two then yields sentential forms  
\[ S \Rightarrow SS \Rightarrow SSS, \]  
\[ S \Rightarrow SS \Rightarrow aSbS, \]  
\[ S \Rightarrow SS \Rightarrow bSaS, \]  
\[ S \Rightarrow SS \Rightarrow S, \]  
which are obtained by replacing the leftmost \( S \) in sentential form 1 with all applicable substitutes. Similarly, from sentential form 2 we get the additional sentential forms  
\[ S \Rightarrow aSb \Rightarrow aSSb, \]  
\[ S \Rightarrow aSb \Rightarrow aaSbb, \]  
\[ S \Rightarrow aSb \Rightarrow abSab, \]  
\[ S \Rightarrow aSb \Rightarrow ab. \]  

Again, several of these can be removed from contention. On the next round, we find the actual target string from the sequence  
\[ S \Rightarrow aSb \Rightarrow aaSbb \Rightarrow aabb. \]  
Therefore, \( aabb \) is in the language generated by the grammar under consideration.

Exhaustive search parsing has serious flaws. The most obvious one is its tediousness; it is not to be used where efficient parsing is required. But even when efficiency is a secondary issue, there is a more pertinent objection. While the method always parses a \( w \in L(G) \), it is possible that it never terminates for strings not in \( L(G) \). This is certainly the case in the previous example; with \( w = abb \), the method will go on producing trial sentential forms indefinitely unless we build into it some way of stopping.

The problem of nontermination of exhaustive search parsing is relatively easy to overcome if we restrict the form that the grammar can have. If we examine Example 5.7, we see that the difficulty comes from the productions \( S \rightarrow \lambda \); this production can be used to decrease the length of successive sentential forms, so that we cannot tell easily when to stop. If we do not have any such productions, then we have many fewer difficulties. In fact, there are two types of productions we want to rule out, those of the form \( A \rightarrow \lambda \) as well as those of the form \( A \rightarrow B \). As we will see in the next chapter, this restriction does not affect the power of the resulting grammars in any significant way.

---

**EXAMPLE 5.8**  
The grammar  
\[ S \rightarrow SS \mid aSb \mid bSa \mid ab \mid ba \]  
satisfies the given requirements. It generates the language in Example 5.7 without the empty string.

Given any \( w \in [a, b]^+ \), the exhaustive search parsing method will always terminate in no more than \( |w| \) rounds. This is clear because the length of the sentential form grows by at least one symbol in each round. After \( |w| \) rounds we have either produced a parsing or we know that \( w \notin L(G) \).

The idea in this example can be generalized and made into a theorem for context-free languages in general.

**THEOREM 5.2**  
Suppose that \( G = (V, T, S, P) \) is a context-free grammar that does not have any rules of the form  
\[ A \rightarrow \lambda, \]  
or  
\[ A \rightarrow B, \]  
where \( A, B \in V \). Then the exhaustive search parsing method can be made into an algorithm that, for any \( w \in \Sigma^* \), either produces a parsing of \( w \) or tells us that no parsing is possible.

**Proof:** For each sentential form, consider both its length and the number of terminal symbols. Each step in the derivation increases at least one of these. Since neither the length of a sentential form nor the number of terminal symbols can exceed \( |w| \), a derivation cannot involve more than 2 \( |w| \) rounds, at which time we either have a successful parsing or \( w \) cannot be generated by the grammar.

While the exhaustive search method gives a theoretical guarantee that parsing can always be done, its practical usefulness is limited because the number of sentential forms generated by it may be excessively large. Exactly how many sentential forms are generated differs from case to case; no precise general result can be established, but we can put some rough upper bounds on it. If we restrict ourselves to leftmost derivations, we can have no more than \( |P| \) sentential forms after one round, no more than \( |P|^2 \) sentential forms after the second round, and so on. In the proof of Theorem 5.2, we observed that parsing cannot involve more than 2 \( |w| \) rounds; therefore, the total number of sentential forms cannot exceed  
\[M = |P| + |P|^2 + \ldots + |p|^{2|w|} \tag{5.2}\]  
\[= O \left( P^{2|w|+1} \right).\]  
This indicates that the work for exhaustive search parsing may grow exponentially with the length of the string, making the cost of the method prohibitive. Of course, Equation (5.2) is only a bound, and often the number of sentential forms is much smaller. Nevertheless, practical observation shows that exhaustive search parsing is very inefficient in most cases.

The construction of more efficient parsing methods for context-free grammars is a complicated matter that belongs to a course on compilers. We will not pursue it here except for some isolated results.

**THEOREM 5.3**  
For every context-free grammar there exists an algorithm that parses any \( w \in L(G) \) in a number of steps proportional to \( |w|^3 \).

There are several known methods to achieve this, but all of them are sufficiently complicated that we cannot even describe them without developing some additional results. In Section 6.3 we will take this question up again briefly. More details can be found in Harrison 1978 and Hopcroft and Ullman 1979. One reason for not pursuing this in detail is that even these algorithms are unsatisfactory. A method in which the work rises with the third power of the length of the string, while better than an exponential algorithm, is still quite inefficient, and a parser based on it would need an excessive amount of time to analyze even a moderately long program. What we would like to have is a parsing method that takes time proportional to the length of the string. We refer to such a method as a linear time parsing algorithm. We do not know any linear time parsing methods for context-free languages in general, but such algorithms can be found for restricted, but important, special cases.

**DEFINITION 5.4**  
A context-free grammar \( G = (V, T, S, P) \) is said to be a simple grammar or s-grammar if all its productions are of the form  
\[A \to ax,\]  
where \( A \in V, a \in T, x \in V^* \), and any pair \((A, a)\) occurs at most once in \( P \).

**EXAMPLE 5.9**  
The grammar  
\[ S \rightarrow aS \mid bSS \mid c \]  
is an s-grammar. The grammar  
\[ S \rightarrow aS \mid bSS \mid aSS \mid c \]  
is not an s-grammar because the pair \((S, a)\) occurs in the two productions \(S \rightarrow aS\) and \(S \rightarrow aSS\).

While s-grammars are quite restrictive, they are of some interest. As we will see in the next section, many features of common programming languages can be described by s-grammars.

If \(G\) is an s-grammar, then any string \(w\) in \(L(G)\) can be parsed with an effort proportional to \(\mid w\mid\). To see this, look at the exhaustive search method and the string \(w = a_1a_2 \cdots a_n\). Since there can be at most one rule with \(S\) on the left, and starting with \(a_1\) on the right, the derivation must begin with  
\[ S \Rightarrow a_1A_1A_2 \cdots A_m. \]  
Next, we substitute for the variable \(A_1\), but since again there is at most one choice, we must have  
\[ S \Rightarrow a_1a_2B_1B_2 \cdots A_2 \cdots A_m. \]  
We see from this that each step produces one terminal symbol and hence the whole process must be completed in no more than \(\mid w\mid\) steps.

---

**Ambiguity in Grammars and Languages**

On the basis of our argument we can claim that given any \( w \in L(G) \), exhaustive search parsing will produce a derivation tree for \( w \). We say “a” derivation tree rather than “the” derivation tree because of the possibility that a number of different derivation trees may exist. This situation is referred to as **ambiguity**.

**DEFINITION 5.5**  
A context-free grammar \( G \) is said to be **ambiguous** if there exists some \( w \in L(G) \) that has at least two distinct derivation trees. Alternatively, ambiguity implies the existence of two or more leftmost or rightmost derivations.

---

**EXAMPLE 5.10**  
The grammar in Example 5.4, with productions \( S \to aSb \ | \ SS \ | \ \lambda \), is ambiguous. The sentence *aabb* has the two derivation trees shown in Figure 5.4.

**FIGURE 5.4**

Ambiguity is a common feature of natural languages, where it is tolerated and dealt with in a variety of ways. In programming languages, where there should be only one interpretation of each statement, ambiguity must be removed when possible. Often we can achieve this by rewriting the grammar in an equivalent, unambiguous form.

**EXAMPLE 5.11**  
Consider the grammar \( G = (V, T, E, P) \) with  
\[ V = \{E, I\}, \]  
\[ T = \{a, b, c, +, *, (,)\}, \]  
and productions  
\[ \begin{array}{ccc}
E & \rightarrow & I, \\
E & \rightarrow & E + E, \\
E & \rightarrow & E^* E, \\
E & \rightarrow & (E), \\
I & \rightarrow & a | b | c. \\
\end{array} \]  
The strings \((a + b) * c\) and \(a * b + c\) are in \(L(G)\). It is easy to see that this grammar generates a restricted subset of arithmetic expressions for C-like programming languages. The grammar is ambiguous. For instance, the string \(a + b * c\) has two different derivation trees, as shown in Figure 5.5.

\[\begin{array}{ccc}
E & \rightarrow & E \\
+ & \rightarrow & E \\
E & \rightarrow & E \\
E & \rightarrow & E \\
I & \rightarrow & E \\
a & I & I \\
b & \rightarrow & c \\
\end{array}\]  
(a)  
(b)  

**FIGURE 5.5** Two derivation trees for \(a + b * c\).

One way to resolve the ambiguity is, as is done in programming manuals, to associate precedence rules with the operators \(+\) and \(*\). Since normally has higher precedence than +, we would take Figure 5.5(a) as the correct parsing as it indicates that \( b \ast c \) is a subexpression to be evaluated before performing the addition. However, this resolution is completely outside the grammar. It is better to rewrite the grammar so that only one parsing is possible.

---

**EXAMPLE 5.12**  
To rewrite the grammar in Example 5.11 we introduce new variables, taking \( V \) as \(\{E, T, F, I\}\), and replacing the productions with  
\[\begin{array}{ccc}
E & \rightarrow & T, \\
T & \rightarrow & F, \\
F & \rightarrow & I, \\
E & \rightarrow & E + T, \\
T & \rightarrow & T^{*} F, \\
F & \rightarrow & (E), \\
I & \rightarrow & a | b | c.
\end{array}\]  
A derivation tree of the sentence \( a + b \ast c \) is shown in Figure 5.6. No other derivation tree is possible for this string: The grammar is unambiguous. It is also equivalent to the grammar in Example 5.11. It is not too hard to justify these claims in this specific instance, but, in general, the questions of whether a given context-free grammar is ambiguous or whether two given context-free grammars are equivalent are very difficult to answer. In fact, we will later show that there are no general algorithms by which these questions can always be resolved.

**FIGURE 5.6**

In the foregoing example the ambiguity came from the grammar in the sense that it could be removed by finding an equivalent unambiguous grammar. In some instances, however, this is not possible because the ambiguity is in the language.

**DEFINITION 5.6**  
If \( L \) is a context-free language for which there exists an unambiguous grammar, then \( L \) is said to be unambiguous. If every grammar that generates \( L \) is ambiguous, then the language is called **inherently ambiguous**.

It is a somewhat difficult matter even to exhibit an inherently ambiguous language. The best we can do here is give an example with some reasonably plausible claim that it is inherently ambiguous.

---

**EXAMPLE 5.13**  
The language  
\[L = \{a^mb^nc^{m}\} \cup \{a^nb^mc^{m}\},\]  
with \( n \) and \( m \) nonnegative, is an inherently ambiguous context-free language.

That \( L \) is context-free is easy to show. Notice that  
\[L = L_1 \cup L_2,\]  
where \( L_1 \) is generated by  
\[\begin{array}{ccc}
S_1 & \rightarrow & S_{1c}|A, \\
A & \rightarrow & aAb|\lambda
\end{array}\]  
and \( L_2 \) is given by an analogous grammar with start symbol \( S_2 \) and productions  
\[\begin{array}{ccc}
S_2 & \rightarrow & aS_2|B, \\
B & \rightarrow & bBc|\lambda.
\end{array}\]  
Then \( L \) is generated by the combination of these two grammars with the additional production  
\[ S \rightarrow S_1 | S_2. \]  
The grammar is ambiguous since the string \( a^n b^n c^n \) has two distinct derivations, one starting with \( S \Rightarrow S_1 \), the other with \( S \Rightarrow S_2 \). It does not, of course, follow from this that \( L \) is inherently ambiguous as there might exist some other unambiguous grammars for it. But in some way \( L_1 \) and \( L_2 \) have conflicting requirements, the first putting a restriction on the number of \( a's \) and \( b's \), while the second does the same for \( b's \) and \( c's \). A few tries will quickly convince you of the impossibility of combining these requirements in a single set of rules that cover the case \( n = m \) uniquely. A rigorous argument, though, is quite technical. One proof can be found in Harrison 1978.

---

**EXERCISES**

1. Show that  
\[S \rightarrow aS|bS|cA\]  
\[A \rightarrow aA|bS\]  
is an s-grammar.

2. Show that every s-grammar is unambiguous.

3. Find an s-grammar for \( L \) (\( aaa*b + ab* \)).

4. Find an s-grammar for \( L = \{a^n b^n : n \geq 2\} \).

5. Find an s-grammar for \( L = \{a^n b^{2n} : n \geq 2\} \).

6. Find an s-grammar for \( L = \{a^n b^{n+1} : n \geq 1\} \).

7. Let \( G = (V, T, S, P) \) be an s-grammar. Give an expression for the maximum size of \( P \) in terms of \( |V| \) and \( |T| \).

8. Show that the following grammar is ambiguous:  
\[ S \rightarrow AB|aaaB, \]  
\[ A \rightarrow a|Aa, \]  
\[ B \rightarrow b. \]

9. Construct an unambiguous grammar equivalent to the grammar in Exercise 8.

10. Give the derivation tree for \( ((a+b) * c + d) \), using the grammar in Example 5.12.

11. Give the derivation tree for \( a * b + ((c + d)) \), using the grammar in Example 5.12.

12. Give the derivation tree for \( (((a + b) * c)) + a + b \), using the grammar in Example 5.12.

13. Show that a regular language cannot be inherently ambiguous.

14. Give an unambiguous grammar that generates the set of all regular expressions on \( \Sigma = \{a, b\} \).

15. Is it possible for a regular grammar to be ambiguous?

16. Show that the language \( L = \{ww^R : w \in \{a, b\}^* \} \) is not inherently ambiguous.

17. Show that the following grammar is ambiguous:  
\[ S \rightarrow aSbS | bSaS | \lambda. \]

18. Show that the grammar in Example 5.4 is ambiguous, but that the language denoted by it is not.

19. Show that the grammar in Example 1.13 is ambiguous.

20. Show that the grammar in Example 5.5 is unambiguous.

21. Use the exhaustive search parsing method to parse the string \( abbbbbb \) with the grammar in Example 5.5. In general, how many rounds will be needed to parse any string \( w \) in this language?

22. Is the string \( aabbababb \) in the language generated by the grammar \( S \rightarrow aSS|b? \)

23. Show that the grammar in Example 1.14 is unambiguous.

24. Prove the following result. Let \( G = (V, T, S, P) \) be a context-free grammar in which every \( A \in V \) occurs on the left side of at most one production. Then \( G \) is unambiguous.

25. Find a grammar equivalent to that in Example 5.5 that satisfies the conditions of Theorem 5.2.

---

**5.3 CONTEXT-FREE GRAMMARS AND PROGRAMMING LANGUAGES**

One of the most important uses of the theory of formal languages is in the definition of programming languages and in the construction of interpreters and compilers for them. The basic problem here is to define a programming language precisely and to use this definition as the starting point for the writing of efficient and reliable translation programs. Both regular and context-free languages are important in achieving this. As we have seen, regular languages are used in the recognition of certain simple patterns that occur in programming languages, but as we argue in the introduction to this chapter, we need context-free languages to model more complicated aspects.

As with most other languages, we can define a programming language by a grammar. It is traditional in writing on programming languages to use a convention for specifying grammars called the *Backus-Naur* form or BNF. This form is in essence the same as the notation we have used here, but the appearance is different. In BNF, variables are enclosed in triangular brackets. Terminal symbols are written without any special marking. BNF also uses subsidiary symbols such as \( \mid \), much in the way we have done. Thus, the grammar in Example 5.12 might appear in BNF as  
\[\langle \text{expression} \rangle ::= \langle \text{term} \rangle | \langle \text{expression} \rangle + \langle \text{term} \rangle,\]  
\[\langle \text{term} \rangle ::= \langle \text{factor} \rangle | \langle \text{term} \rangle^* \langle \text{factor} \rangle,\]  
and so on. The symbols \( + \) and \( * \) are terminals. The symbol \( \mid \) is used as an alternator as in our notation, but ::= is used instead of →. BNF descriptions of programming languages tend to use more explicit variable identifiers to make the intent of the production explicit. But otherwise there are no significant differences between the two notations.

Many parts of C-like programming languages are susceptible to definition by restricted forms of context-free grammars. For example, the *while* statement in C can be defined as  
\[\langle \text{while statement} \rangle ::= \text{while} \langle \text{expression} \rangle\]  
\[\langle \text{statement} \rangle.\]  
Here the keyword *while* is a terminal symbol. All other terms are variables, which still have to be defined. If we check this against Definition 5.4, we see that this looks like an s-grammar production. The variable \(\langle \text{while_statement} \rangle\) on the left is always associated with the terminal *while* on the right. For this reason such a statement is easily and efficiently parsed. We see here a reason why we use keywords in programming languages. Keywords not only provide some visual structure that can guide the reader of a program, but also make the work of a compiler much easier.

Unfortunately, not all features of a typical programming language can be expressed by an s-grammar. The rules for \(\langle \text{expression} \rangle\) above are not of this type, so that parsing becomes less obvious. The question then arises what grammatical rules we can permit and still parse efficiently. In compilers, extensive use has been made of what are called LL and LR grammars. These grammars have the ability to express the less obvious features of a programming language, yet allow us to parse in linear time. This is not a simple matter, and much of it is beyond the scope of our discussion. We will briefly touch on this topic in Chapter 6, but for our purposes it suffices to realize that such grammars exist and have been widely studied.

In connection with this, the issue of ambiguity takes on added significance. The specification of a programming language must be unambiguous, otherwise a program may yield very different results when processed by different compilers or run on different systems. As Example 5.11 shows, a naive approach can easily introduce ambiguity in the grammar. To avoid such mistakes we must be able to recognize and remove ambiguities. A related question is whether a language is or is not inherently ambiguous. What we need for this purpose are algorithms for detecting and removing ambiguities in context-free grammars and for deciding whether or not a context-free language is inherently ambiguous. Unfortunately, these are very difficult tasks, impossible in the most general sense, as we will see later.

Those aspects of a programming language that can be modeled by a context-free grammar are usually referred to as its syntax. However, it is normally the case that not all programs that are syntactically correct in this sense are in fact acceptable programs. For C, the usual BNF definition allows constructs such as  
\[char \, a, \, b, \, c;\]  
followed by  
\[c = 3.2;\]  
This combination is not acceptable to C compilers since it violates the constraint, “a character variable cannot be assigned a real value.” Context-free grammars cannot express the fact that type clashes may not be permitted. Such rules are part of programming language semantics, since they have to do with how we interpret the meaning of a particular construct.

Programming language semantics are a complicated matter. Nothing as elegant and concise as context-free grammars exists for the specification of programming language semantics, and consequently some semantic features may be poorly defined or ambiguous. It is an ongoing concern both in programming languages and in formal language theory to find effective methods for defining programming language semantics. Several methods have been proposed, but none of them has been as universally accepted and are as successful for semantic definition as context-free languages have been for syntax.

---

**EXERCISES**

1. Suppose that in a certain programming language numbers are constructed according to the following rules:  
   (a) A sign is optional.